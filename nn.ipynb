{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNeuralNetwork(object):\n",
    "    def __init__(self, n_in=2, n_layer1=5, n_layer2=5, n_out=1, learning_rate=0.01):\n",
    "        \"\"\"__init__\n",
    "        Class constructor: Initialize the parameters of the network including\n",
    "        the learning rate, layer sizes, and each of the parameters\n",
    "        of the model (weights, placeholders for activations, inputs,\n",
    "        deltas for gradients, and weight gradients). This method\n",
    "        should also initialize the weights of your model randomly\n",
    "            Input:\n",
    "                n_in:          number of inputs\n",
    "                n_layer1:      number of nodes in layer 1\n",
    "                n_layer2:      number of nodes in layer 2\n",
    "                n_out:         number of output nodes\n",
    "                learning_rate: learning rate for gradient descent\n",
    "            Output:\n",
    "                none\n",
    "        \"\"\"\n",
    "        self.n_in = n_in\n",
    "        self.n_layer1 = n_layer1\n",
    "        self.n_layer2 = n_layer2\n",
    "        self.n_out = n_out\n",
    "        self.learning_rate = learning_rate\n",
    "        # initialize weights\n",
    "        self.W1 = np.random.randn(n_layer1, n_in)\n",
    "        self.W2 = np.random.randn(n_layer2, n_layer1)\n",
    "        self.W3 = np.random.randn(n_out, n_layer2)\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        \"\"\"forward_propagation\n",
    "        Takes a vector of your input data (one sample) and feeds\n",
    "        it forward through the neural network, calculating activations and\n",
    "        layer node values along the way.\n",
    "            Input:\n",
    "                x: a vector of data representing 1 sample [n_in x 1]\n",
    "            Output:\n",
    "                y_hat: a vector (or scaler of predictions) [n_out x 1]\n",
    "                (typically n_out will be 1 for binary classification)\n",
    "        \"\"\"\n",
    "\n",
    "        # input layer -> hidden layer 1\n",
    "        self.a1 = np.dot(self.W1, x.T)\n",
    "        self.z1 = self.sigmoid(self.a1)\n",
    "\n",
    "        # hidden layer 1 -> hidden layer 2\n",
    "        self.a2 = self.W2 @ self.z1\n",
    "        self.z2 = self.sigmoid(self.a2)\n",
    "\n",
    "        # hidden layer 2 -> output layer\n",
    "        self.a3 = self.W3 @ self.z2\n",
    "        z3 = self.sigmoid(self.a3)\n",
    "        return z3\n",
    "\n",
    "    def compute_loss(self, y_hat, y):\n",
    "        \"\"\"compute_loss\n",
    "        Computes the current loss/cost function of the neural network\n",
    "        based on the weights and the data input into this function.\n",
    "        To do so, it runs the X data through the network to generate\n",
    "        predictions, then compares it to the target variable y using\n",
    "        the cost/loss function\n",
    "            Input:\n",
    "                y_hat: The estimation based on the forward propagation\n",
    "                y: Target variable\n",
    "            Output:\n",
    "                loss: a scalar measure of loss/cost\n",
    "        \"\"\"\n",
    "        # another way to compute the loss but takes forever\n",
    "        # mean squared error loss function\n",
    "        # y_hat = self.forward_propagation(X)\n",
    "        # return np.mean(0.5 * (y - y_hat)**2)\n",
    "        return 0.5 * (y_hat - y) ** 2\n",
    "\n",
    "    def backpropagate(self, x, y):\n",
    "        \"\"\"backpropagate\n",
    "        Backpropagate the error from one sample determining the gradients\n",
    "        with respect to each of the weights in the network. The steps for\n",
    "        this algorithm are:\n",
    "            1. Run a forward pass of the model to get the activations\n",
    "               Corresponding to x and get the loss functionof the model\n",
    "               predictions compared to the target variable y\n",
    "            2. Compute the deltas (see lecture notes) and values of the\n",
    "               gradient with respect to each weight in each layer moving\n",
    "               backwards through the network\n",
    "\n",
    "            Input:\n",
    "                x: A vector of 1 samples of data [n_in x 1]\n",
    "                y: Target variable [scalar]\n",
    "            Output:\n",
    "                loss: a scalar measure of th loss/cost associated with x,y\n",
    "                      and the current model weights\n",
    "        \"\"\"\n",
    "        # using gradient descent to update the weights\n",
    "        y_hat = self.forward_propagation(x)\n",
    "        # output layer -> hidden layer 2\n",
    "        delta3 = -(y - y_hat) * self.sigmoid_derivative(self.a3)\n",
    "        dW3 = delta3[np.newaxis].T @ self.z2[np.newaxis]\n",
    "        # hidden layer 2 -> hidden layer 1\n",
    "        delta2 = np.dot(self.W3.T, delta3) * self.sigmoid_derivative(self.a2)\n",
    "        dW2 = delta2[np.newaxis].T @ self.z1[np.newaxis]\n",
    "        # hidden layer 1 -> input layer\n",
    "        delta1 = np.dot(self.W2.T, delta2) * self.sigmoid_derivative(self.a1)\n",
    "        dW1 = delta1[np.newaxis].T @ x[np.newaxis]\n",
    "\n",
    "        # Update weights\n",
    "        self.W3 -= self.learning_rate * dW3\n",
    "        self.W2 -= self.learning_rate * dW2\n",
    "        self.W1 -= self.learning_rate * dW1\n",
    "        # Calculate loss\n",
    "        loss = self.compute_loss(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def stochastic_gradient_descent_step(self, X, y):\n",
    "        \"\"\"stochastic_gradient_descent_step - performs a single step of\n",
    "        stochastic gradient descent on the training data\n",
    "        by randomly shuffling the training data and testing each\n",
    "\n",
    "        Input: X: A matrix of N samples of data [N x n_in]\n",
    "                y: Target variable [N x 1]\n",
    "        Output: randomly shuffle the training data\n",
    "        \"\"\"\n",
    "        # another way to update the weights\n",
    "        # randomly shuffle the training data\n",
    "        # idx = np.random.permutation(X.shape[0])\n",
    "        # for i in idx:\n",
    "        #     X_i  = X[i].reshape(-1,1)\n",
    "        #     self.backpropagate(X_i,y[i])\n",
    "        idx = random.choices(range(0, X.shape[0]), k=X.shape[0])\n",
    "        return X[idx], y[idx]\n",
    "\n",
    "    def fit(self, X, y, X_val, y_val, max_epochs=500, learning_rate=0.01, get_validation_loss=False):\n",
    "        \"\"\"fit\n",
    "        Input:\n",
    "            X: A matrix of N samples of data [N x n_in]\n",
    "            y: Target variable [N x 1]\n",
    "        Output:\n",
    "            training_loss:   Vector of training loss values at the end of each epoch\n",
    "            validation_loss: Vector of validation loss values at the end of each epoch\n",
    "                             [optional output if get_validation_loss==True]\n",
    "        \"\"\"\n",
    "        self.cost_function = []\n",
    "        self.weights = []\n",
    "        training_loss = []\n",
    "        validation_loss = []\n",
    "\n",
    "        for epoch in range(max_epochs):\n",
    "            X_rand, y_rand = self.stochastic_gradient_descent_step(\n",
    "                X, y)\n",
    "            for i, j in zip(X_rand, y_rand):\n",
    "                t_loss = self.backpropagate(i, j)\n",
    "                training_loss.append(t_loss)\n",
    "            self.cost_function.append(sum(training_loss) / len(training_loss))\n",
    "            self.weights.append([self.W1, self.W2, self.W3])\n",
    "            value_min = min(self.cost_function)\n",
    "            index_min = self.cost_function.index(value_min)\n",
    "            self.W1 = self.weights[index_min][0]\n",
    "            self.W2 = self.weights[index_min][1]\n",
    "            self.W3 = self.weights[index_min][2]\n",
    "            if get_validation_loss:\n",
    "                y_hat_val = self.predict_proba(X_val)\n",
    "                v_loss = self.compute_loss(y_hat_val, y_val)\n",
    "                validation_loss.append(np.mean(v_loss))\n",
    "                return training_loss, validation_loss\n",
    "            else:\n",
    "                return training_loss\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"predict_proba\n",
    "        Compute the output of the neural network for each sample in X, with the last layer's\n",
    "        sigmoid activation providing an estimate of the target output between 0 and 1\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "            Output:\n",
    "                y_hat: A vector of class predictions between 0 and 1 [N x 1]\n",
    "        \"\"\"\n",
    "        return self.forward_propagation(X)\n",
    "\n",
    "    def predict(self, X, decision_thresh=0.5):\n",
    "        \"\"\"predict\n",
    "        Compute the output of the neural network prediction for\n",
    "        each sample in X, with the last layer's sigmoid activation\n",
    "        providing an estimate of the target output between 0 and 1,\n",
    "        then thresholding that prediction based on decision_thresh\n",
    "        to produce a binary class prediction\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "                decision_threshold: threshold for the class confidence score\n",
    "                                    of predict_proba for binarizing the output\n",
    "            Output:\n",
    "                y_hat: A vector of class predictions of either 0 or 1 [N x 1]\n",
    "        \"\"\"\n",
    "        return (self.forward_propagation(X) > decision_thresh).astype(int)\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        \"\"\"sigmoid\n",
    "        Compute the sigmoid function for each value in matrix X\n",
    "            Input:\n",
    "                X: A matrix of any size [m x n]\n",
    "            Output:\n",
    "                X_sigmoid: A matrix [m x n] where each entry corresponds to the\n",
    "                           entry of X after applying the sigmoid function\n",
    "        \"\"\"\n",
    "        # Our activation function: f(x) = 1 / (1 + e^(-x))\n",
    "        return 1 / (1 + np.exp(-X))\n",
    "\n",
    "    def sigmoid_derivative(self, X):\n",
    "        \"\"\"sigmoid_derivative\n",
    "        Compute the sigmoid derivative function for each value in matrix X\n",
    "            Input:\n",
    "                X: A matrix of any size [m x n]\n",
    "            Output:\n",
    "                X_sigmoid: A matrix [m x n] where each entry corresponds to the\n",
    "                           entry of X after applying the sigmoid derivative function\n",
    "        \"\"\"\n",
    "        # sigmoid derivative: f'(x) = f(x) * (1 - f(x))\n",
    "        return self.sigmoid(X) * (1 - self.sigmoid(X))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNeuralNetwork():\n",
    "\n",
    "    def __init__(self, n_in=2, n_layer1=5, n_layer2=5, n_out=1, learning_rate=0.01):\n",
    "        '''__init__\n",
    "        Class constructor: Initialize the parameters of the network including\n",
    "        the learning rate, layer sizes, and each of the parameters\n",
    "        of the model (weights, placeholders for activations, inputs, \n",
    "        deltas for gradients, and weight gradients). This method\n",
    "        should also initialize the weights of your model randomly\n",
    "            Input:\n",
    "                n_in:          number of inputs\n",
    "                n_layer1:      number of nodes in layer 1\n",
    "                n_layer2:      number of nodes in layer 2\n",
    "                n_out:         number of output nodes\n",
    "                learning_rate: learning rate for gradient descent\n",
    "            Output:\n",
    "                none\n",
    "        '''\n",
    "        self.cost_function = []\n",
    "        self.validation_loss = []\n",
    "        self.weights = []\n",
    "        self.lr = learning_rate\n",
    "        # Initiate weights\n",
    "        self.w_1 = self.generate_wt(n_layer1, n_in)\n",
    "        self.w_2 = self.generate_wt(n_layer2, n_layer1)\n",
    "        self.w_3 = self.generate_wt(n_out, n_layer2)\n",
    "        pass\n",
    "\n",
    "    def forward_propagation(self, x):\n",
    "        '''forward_propagation\n",
    "        Takes a vector of your input data (one sample) and feeds\n",
    "        it forward through the neural network, calculating activations and\n",
    "        layer node values along the way.\n",
    "            Input:\n",
    "                x: a vector of data representing 1 sample [n_in x 1]\n",
    "            Output:\n",
    "                y_hat: a vector (or scaler of predictions) [n_out x 1]\n",
    "                (typically n_out will be 1 for binary classification)\n",
    "        '''\n",
    "        # Input - Layer 1\n",
    "        self.a_1 = np.dot(self.w_1, x.T)\n",
    "        self.z_1 = self.sigmoid(self.a_1)\n",
    "\n",
    "        # Layer 1 - Layer 2\n",
    "        self.a_2 = self.w_2 @ self.z_1\n",
    "        self.z_2 = self.sigmoid(self.a_1)\n",
    "\n",
    "        # Output layer\n",
    "        self.a_3 = self.w_3 @ self.z_2\n",
    "        y_hat = self.sigmoid(self.a_3)\n",
    "        return y_hat\n",
    "\n",
    "    def compute_loss(self, y_hat, y):\n",
    "        '''compute_loss\n",
    "        Computes the current loss/cost function of the neural network\n",
    "        based on the weights and the data input into this function.\n",
    "        To do so, it runs the X data through the network to generate\n",
    "        predictions, then compares it to the target variable y using\n",
    "        the cost/loss function\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "                y: Target variable [N x 1]\n",
    "            Output:\n",
    "                loss: a scalar measure of loss/cost\n",
    "        '''\n",
    "        return 0.5 * (y_hat - y) ** 2\n",
    "\n",
    "    def backpropagate(self, x, y):\n",
    "        '''backpropagate\n",
    "        Backpropagate the error from one sample determining the gradients\n",
    "        with respect to each of the weights in the network. The steps for\n",
    "        this algorithm are:\n",
    "            1. Run a forward pass of the model to get the activations \n",
    "               Corresponding to x and get the loss functionof the model \n",
    "               predictions compared to the target variable y\n",
    "            2. Compute the deltas (see lecture notes) and values of the\n",
    "               gradient with respect to each weight in each layer moving\n",
    "               backwards through the network\n",
    "\n",
    "            Input:\n",
    "                x: A vector of 1 samples of data [n_in x 1]\n",
    "                y: Target variable [scalar]\n",
    "            Output:\n",
    "                loss: a scalar measure of th loss/cost associated with x,y\n",
    "                      and the current model weights\n",
    "        '''\n",
    "        y_hat = self.forward_propagation(x)\n",
    "\n",
    "        # Compute deltas\n",
    "        delta_3 = ((y_hat-y) * self.sigmoid_derivative(self.a_3))\n",
    "        delta_2 = np.multiply((self.w_3.T @ delta_3),\n",
    "                              self.sigmoid_derivative(self.a_2))\n",
    "        delta_1 = np.multiply((self.w_2.T @ delta_2),\n",
    "                              self.sigmoid_derivative(self.a_1))\n",
    "\n",
    "        # Compute gradients\n",
    "        grad_w_1 = delta_1[np.newaxis].T @ x[np.newaxis]\n",
    "        grad_w_2 = delta_2[np.newaxis].T @ self.z_1[np.newaxis]\n",
    "        grad_w_3 = delta_3[np.newaxis].T @ self.z_2[np.newaxis]\n",
    "\n",
    "        # Gradient descent step: NEW WEIGHTZZZ\n",
    "        self.w_1 = self.w_1 - (self.lr * grad_w_1)\n",
    "        self.w_2 = self.w_2 - (self.lr * grad_w_2)\n",
    "        self.w_3 = self.w_3 - (self.lr * grad_w_3)\n",
    "\n",
    "        return self.compute_loss(y_hat, y)\n",
    "\n",
    "    def fit(self, X, y, X_val, y_val, max_epochs=500, get_validation_loss=True):\n",
    "        '''fit\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "                y: Target variable [N x 1]\n",
    "            Output:\n",
    "                training_loss:   Vector of training loss values at the end of each epoch\n",
    "                validation_loss: Vector of validation loss values at the end of each epoch\n",
    "                                 [optional output if get_validation_loss==True]\n",
    "        '''\n",
    "        for epoch in range(max_epochs):\n",
    "            X_rand, y_rand = self.randomized(X, y)\n",
    "            epoch_loss = []\n",
    "\n",
    "            # Iterations on each epoch\n",
    "            for i, j in zip(X_rand, y_rand):\n",
    "                loss = self.backpropagate(i, j)\n",
    "                epoch_loss.append(loss)\n",
    "                # did a forward pass an calculated gradients\n",
    "                # Also computed new weights\n",
    "                # And, saved the new cost in the list\n",
    "            # 1 EPOCH IS DONE!\n",
    "            self.cost_function.append(sum(epoch_loss)/len(epoch_loss))\n",
    "            self.weights.append([self.w_1, self.w_2, self.w_3])\n",
    "            if get_validation_loss == True:\n",
    "\n",
    "                # print('Mean Loss at epoch {} = {}'.format(epoch, sum(epoch_loss)/len(epoch_loss)))\n",
    "\n",
    "                y_hat_val = self.predict_proba(X_val)\n",
    "                cost_val = self.compute_loss(y_hat_val, y_val)\n",
    "                self.validation_loss.append(np.mean(cost_val))\n",
    "        min_value = min(self.cost_function)\n",
    "        min_index = self.cost_function.index(min_value)\n",
    "        self.w_1 = self.weights[min_index][0]\n",
    "        self.w_2 = self.weights[min_index][1]\n",
    "        self.w_3 = self.weights[min_index][2]\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        '''predict_proba\n",
    "        Compute the output of the neural network for each sample in X, with the last layer's\n",
    "        sigmoid activation providing an estimate of the target output between 0 and 1\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "            Output:\n",
    "                y_hat: A vector of class predictions between 0 and 1 [N x 1]\n",
    "        '''\n",
    "        return self.forward_propagation(X)\n",
    "\n",
    "    def predict(self, X, decision_thresh=0.5):\n",
    "        '''predict\n",
    "        Compute the output of the neural network prediction for \n",
    "        each sample in X, with the last layer's sigmoid activation \n",
    "        providing an estimate of the target output between 0 and 1, \n",
    "        then thresholding that prediction based on decision_thresh\n",
    "        to produce a binary class prediction\n",
    "            Input:\n",
    "                X: A matrix of N samples of data [N x n_in]\n",
    "                decision_threshold: threshold for the class confidence score\n",
    "                                    of predict_proba for binarizing the output\n",
    "            Output:\n",
    "                y_hat: A vector of class predictions of either 0 or 1 [N x 1]\n",
    "        '''\n",
    "\n",
    "        return self.predict_proba(X) >= decision_thresh\n",
    "\n",
    "    def sigmoid(self, X):\n",
    "        '''sigmoid\n",
    "        Compute the sigmoid function for each value in matrix X\n",
    "            Input:\n",
    "                X: A matrix of any size [m x n]\n",
    "            Output:\n",
    "                X_sigmoid: A matrix [m x n] where each entry corresponds to the\n",
    "                           entry of X after applying the sigmoid function\n",
    "        '''\n",
    "        return 1/(1+np.exp(-X))\n",
    "\n",
    "    def sigmoid_derivative(self, X):\n",
    "        '''sigmoid_derivative\n",
    "        Compute the sigmoid derivative function for each value in matrix X\n",
    "            Input:\n",
    "                X: A matrix of any size [m x n]\n",
    "            Output:\n",
    "                X_sigmoid: A matrix [m x n] where each entry corresponds to the\n",
    "                           entry of X after applying the sigmoid derivative function\n",
    "        '''\n",
    "        sig = self.sigmoid(X)\n",
    "        return sig * (1 - sig)\n",
    "\n",
    "    def generate_wt(self, a, b):\n",
    "        ''' Initialize weights of any size'''\n",
    "        l = []\n",
    "        for i in range(a * b):\n",
    "            l.append(np.random.randn())\n",
    "        return (np.array(l).reshape(a, b))\n",
    "\n",
    "    def randomized(self, X, y):\n",
    "        idx = random.choices(range(0, X.shape[0]), k=X.shape[0])\n",
    "        return X[idx], y[idx]\n",
    "\n",
    "    def fix_shape(self, array):\n",
    "        return array[np.newaxis].T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set layer and node based on the previous results\n",
    "# lr = learning_rates[np.argmax(lr_accuracy)]\n",
    "# reg = regularization[np.argmax(reg_accuracy)]\n",
    "# batch = batch_sizes[np.argmax(batch_accuracy)]\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, roc_curve, accuracy_score\n",
    "\n",
    "layer = (10, 10)\n",
    "mlp_manual = mlp_classifer(\n",
    "    hidden_layer_sizes=layer, learning_rate_init=lr, alpha=reg, batch_size=batch\n",
    ")\n",
    "print(\n",
    "    \"The manual best model has an accuracy of {} when we test in testing data\".format(\n",
    "        mlp_manual.score(X_test, y_test)\n",
    "    )\n",
    ")\n",
    "# Define the hyperparameters\n",
    "params = {\n",
    "    \"hidden_layer_sizes\": (2, 10),\n",
    "    \"learning_rate_init\": lr,\n",
    "    \"alpha\": reg,\n",
    "    \"solver\": \"sgd\",\n",
    "    \"tol\": 1e-5,\n",
    "    \"early_stopping\": False,\n",
    "    \"activation\": \"relu\",\n",
    "    \"n_iter_no_change\": 1000,\n",
    "    \"batch_size\": batch,\n",
    "    \"max_iter\": 500,\n",
    "}\n",
    "\n",
    "# Train the model on all the training and validation data\n",
    "model_greedy = MLPClassifier(**params)\n",
    "model_greedy.fit(X_train_plus_val, y_train_plus_val)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = model_greedy.predict(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test set accuracy: {:.3f}\".format(test_acc))\n",
    "\n",
    "# Compute the ROC curve and AUC score\n",
    "y_prob = model_greedy.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "# Plot the ROC curve\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(fpr, tpr, label=\"ROC curve (area = {:.3f})\".format(auc))\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\", label=\"Random guessing\")\n",
    "ax.set_xlabel(\"False Positive Rate\")\n",
    "ax.set_ylabel(\"True Positive Rate\")\n",
    "ax.set_title(\"Receiver operating characteristic (ROC) curve\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the grid search parameters\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001],\n",
    "    \"max_epochs\": [500, 1000, 2000, 3000],\n",
    "}\n",
    "for learning_rate in param_grid[\"learning_rate\"]:\n",
    "    for max_epochs in param_grid[\"max_epochs\"]:\n",
    "        nn = myNeuralNetwork(learning_rate=learning_rate)\n",
    "        nn.fit(\n",
    "            X_train_moon, y_train_moon, X_val_moon, y_val_moon, max_epochs=max_epochs\n",
    "        )\n",
    "        training_loss = nn.cost_function\n",
    "        plt.plot(\n",
    "            training_loss,\n",
    "            label=\"Learning rate: \"\n",
    "            + str(learning_rate)\n",
    "            + \", Epochs: \"\n",
    "            + str(max_epochs),\n",
    "        )\n",
    "        print(\n",
    "            \"Learning rate: \" + str(learning_rate) +\n",
    "            \", Epochs: \" + str(max_epochs),\n",
    "            \"Training loss: \",\n",
    "            training_loss[-1],\n",
    "        )\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.title(\"Learning curves for different hyperparameters\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create training, validation, and test datasets\n",
    "N_train = 500\n",
    "N_test = 100\n",
    "X, y = make_moons(N_train + N_test, noise=0.20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=N_test, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0497155  1.18000638] 0\n",
      "[ 0.28286373 -0.49823475] 1\n",
      "[-0.99717956  0.36625365] 0\n",
      "[1.43490257 0.35214387] 1\n",
      "[-1.17294886  0.85348354] 0\n",
      "[-0.8697229   0.33448305] 0\n",
      "[0.76646791 1.15635944] 0\n",
      "[-1.04768639  0.83954361] 0\n",
      "[1.71129945 0.04543038] 1\n",
      "[-0.599014    0.62319734] 0\n",
      "[-0.98701414  0.41535375] 0\n",
      "[-0.0560974   0.08494591] 1\n",
      "[0.03271477 0.85611776] 0\n",
      "[-0.21763375  0.85907536] 0\n",
      "[1.12665947 0.44571536] 0\n",
      "[1.03097555 0.37777989] 0\n",
      "[ 1.71753261 -0.17130622] 1\n",
      "[1.04488858 0.12152307] 0\n",
      "[-0.66673136  0.67101844] 0\n",
      "[ 0.58413807 -0.52083984] 1\n",
      "[-0.08373787  0.9689945 ] 0\n",
      "[-0.46181582  1.0341428 ] 0\n",
      "[0.49434053 0.8994933 ] 0\n",
      "[ 1.5950776  -0.66355318] 1\n",
      "[0.07540208 0.30977001] 1\n",
      "[1.50700603 0.28939968] 1\n",
      "[ 1.73874125 -0.05885145] 1\n",
      "[ 1.74614445 -0.23696991] 1\n",
      "[0.4485357  0.67418395] 0\n",
      "[ 1.49554928 -0.14240095] 1\n",
      "[-0.01465594 -0.15638416] 1\n",
      "[ 0.47110739 -0.39465297] 1\n",
      "[0.38032726 0.11657802] 1\n",
      "[-0.95226787  0.45558919] 0\n",
      "[0.1373189  0.22693396] 1\n",
      "[ 0.70407854 -0.33479636] 1\n",
      "[-0.36880698  1.20951131] 0\n",
      "[ 0.69931465 -0.25067368] 1\n",
      "[1.93936805 0.12338317] 1\n",
      "[0.32649462 1.40197042] 0\n",
      "[1.09221613 0.20027194] 0\n",
      "[ 0.90132135 -0.41206861] 1\n",
      "[0.88340868 0.00204865] 0\n",
      "[1.97327121 0.13828297] 1\n",
      "[ 1.18882702 -0.12322082] 0\n",
      "[0.31248511 1.0075456 ] 0\n",
      "[ 1.28232252 -0.63479938] 1\n",
      "[0.54065026 0.87540855] 0\n",
      "[0.11473315 0.24413543] 1\n",
      "[ 2.35963934 -0.09580322] 1\n",
      "[-0.27062921  0.90924594] 0\n",
      "[0.87903422 0.60155937] 0\n",
      "[-0.90610786  0.07534619] 0\n",
      "[ 1.23766348 -0.67029036] 1\n",
      "[0.86743461 0.67688353] 0\n",
      "[-0.06174832  0.48588223] 1\n",
      "[-0.7706839   0.35014047] 0\n",
      "[1.88148931 0.30510709] 1\n",
      "[-1.07116508  0.28631007] 0\n",
      "[-0.10836063  0.87011238] 0\n",
      "[0.67802792 0.94724349] 0\n",
      "[-1.26317993  0.31738984] 0\n",
      "[ 1.06496041 -0.42432086] 1\n",
      "[-1.02905582  0.72646525] 0\n",
      "[ 0.4588022  -0.55041716] 1\n",
      "[ 0.52326961 -0.08580097] 1\n",
      "[ 0.65301628 -0.6525999 ] 1\n",
      "[-0.31609086  0.71050193] 0\n",
      "[ 1.6294624  -0.19191543] 1\n",
      "[ 0.5086507  -0.51098254] 1\n",
      "[ 2.07524402 -0.09714568] 1\n",
      "[2.28685419 0.32814423] 1\n",
      "[-0.84326722  1.06820842] 0\n",
      "[1.12867079 0.31294092] 0\n",
      "[1.50969444 0.10314493] 1\n",
      "[0.94337409 0.90179268] 0\n",
      "[ 0.53712754 -0.46452172] 1\n",
      "[ 1.01516921 -0.19390364] 1\n",
      "[ 0.26736726 -0.54930931] 1\n",
      "[ 1.33608761 -0.27534286] 1\n",
      "[ 0.76936822 -0.47083045] 1\n",
      "[0.53562996 0.84851659] 0\n",
      "[ 0.99312512 -0.39882181] 1\n",
      "[ 0.97244211 -0.34642342] 1\n",
      "[ 1.45145333 -0.4157194 ] 1\n",
      "[-1.14291079  0.11014742] 0\n",
      "[-0.66462398  0.66308768] 0\n",
      "[0.99502086 0.04112282] 0\n",
      "[0.88850537 0.32098474] 0\n",
      "[ 1.28809822 -0.41427961] 1\n",
      "[-9.58962479e-01  5.89667606e-04] 0\n",
      "[-0.84202599  0.38743079] 0\n",
      "[ 1.66827446 -0.4444165 ] 1\n",
      "[ 1.99348527 -0.29279023] 1\n",
      "[0.11399542 0.1409837 ] 1\n",
      "[-0.64692807  0.97440067] 0\n",
      "[1.59760182 0.16507129] 1\n",
      "[ 0.87171969 -0.44248445] 1\n",
      "[ 1.03265009 -0.71228576] 1\n",
      "[0.55989132 0.78315713] 0\n",
      "[ 1.76865796 -0.23135879] 1\n",
      "[ 1.21936143 -0.24914332] 1\n",
      "[ 1.05855861 -0.47125131] 1\n",
      "[1.85862477 0.17802083] 1\n",
      "[-1.16672978  0.12123458] 0\n",
      "[1.05699451 0.49040508] 0\n",
      "[ 0.51030915 -0.72166146] 1\n",
      "[-0.1996979   1.30122952] 0\n",
      "[-1.08519947  0.51188389] 0\n",
      "[ 1.6419365  -0.31632507] 1\n",
      "[ 0.47021989 -0.66376653] 1\n",
      "[ 0.86991033 -0.13644856] 0\n",
      "[1.20379949 0.46113846] 0\n",
      "[ 0.36242781 -0.01648799] 1\n",
      "[0.62790604 0.86339672] 0\n",
      "[ 1.47429178 -0.40319399] 1\n",
      "[1.69677619 0.06366275] 1\n",
      "[0.31525369 0.12333529] 1\n",
      "[-0.84775702  1.16087882] 0\n",
      "[-0.9111939   0.32272485] 0\n",
      "[-0.83228021  0.90445659] 0\n",
      "[ 0.95454581 -0.5928897 ] 1\n",
      "[0.10809497 0.06674957] 1\n",
      "[ 1.71501492 -0.30638772] 1\n",
      "[0.14192579 1.31404286] 0\n",
      "[-0.87491687  0.33823251] 0\n",
      "[ 0.11798765 -0.40960254] 1\n",
      "[ 0.86577255 -0.84283057] 1\n",
      "[-0.64412804  0.57698238] 0\n",
      "[-0.46069847  0.6417433 ] 0\n",
      "[0.02145974 0.06775884] 1\n",
      "[ 0.5284883  -0.58606181] 1\n",
      "[-0.44709146  0.53214408] 0\n",
      "[0.76299399 0.51363601] 0\n",
      "[0.64978009 1.10299158] 0\n",
      "[ 0.52144692 -0.52255045] 1\n",
      "[1.03020242 0.53516241] 0\n",
      "[-0.44264455  0.77893597] 0\n",
      "[1.84880125 0.04140756] 1\n",
      "[ 1.60075054 -0.6839772 ] 1\n",
      "[0.28835638 1.21714631] 0\n",
      "[ 0.12391501 -0.09994488] 1\n",
      "[-1.04132036  0.3742193 ] 0\n",
      "[ 1.44315218 -0.12077409] 1\n",
      "[-0.25899533  0.7457603 ] 0\n",
      "[-0.61290757  1.17837123] 0\n",
      "[2.18053075 0.64458988] 1\n",
      "[-0.36090054  0.77071048] 0\n",
      "[-0.3177962   0.38291127] 1\n",
      "[ 1.29044063 -0.27062437] 1\n",
      "[0.43982505 1.20245812] 0\n",
      "[-0.28307041  0.97840172] 0\n",
      "[ 1.15623762 -0.11314516] 1\n",
      "[1.13876658 0.46258113] 0\n",
      "[-0.15938242  0.14727675] 1\n",
      "[0.13037885 0.51110007] 1\n",
      "[-0.81909962  0.35609292] 0\n",
      "[-0.00836175  0.08099135] 1\n",
      "[0.5517162  0.61516409] 0\n",
      "[0.78388213 0.75219271] 0\n",
      "[1.16217957 0.04855994] 0\n",
      "[0.95460026 0.30933867] 0\n",
      "[ 0.35400932 -0.33701178] 1\n",
      "[2.16077674 0.16098552] 1\n",
      "[ 1.48485206 -0.34256446] 1\n",
      "[-0.1623408   0.73148142] 1\n",
      "[0.58487768 0.98286468] 0\n",
      "[-0.87578974  0.63454151] 0\n",
      "[-0.38769979  1.07791933] 0\n",
      "[ 1.52497428 -0.28293974] 1\n",
      "[ 0.42528036 -0.16829173] 1\n",
      "[ 0.22360698 -0.34224604] 1\n",
      "[2.06378275 0.11957926] 1\n",
      "[1.85873789 0.2586839 ] 1\n",
      "[-0.8104602   0.44988136] 0\n",
      "[ 2.18570846 -0.29342133] 1\n",
      "[0.17353897 0.09762829] 1\n",
      "[0.29714942 1.14786335] 0\n",
      "[0.39477657 0.28134987] 1\n",
      "[0.71515769 0.86957201] 0\n",
      "[0.83518656 0.06535002] 1\n",
      "[1.14400523 0.14311342] 0\n",
      "[ 0.86520541 -0.41074258] 1\n",
      "[ 1.69796167 -0.17686602] 1\n",
      "[-0.93760598  0.37611544] 0\n",
      "[ 1.58584066 -0.41472768] 1\n",
      "[0.7443397  0.91521724] 0\n",
      "[1.14944895 0.2807359 ] 0\n",
      "[ 0.50081106 -0.38338096] 1\n",
      "[ 1.7015978  -0.28661115] 1\n",
      "[2.15253195 0.18348706] 1\n",
      "[1.22467548 0.86766472] 0\n",
      "[ 1.02254482 -0.01074013] 0\n",
      "[0.45380653 0.25149246] 1\n",
      "[0.79467237 0.3867434 ] 0\n",
      "[ 0.17032545 -0.06988321] 1\n",
      "[ 0.35160916 -0.13648354] 1\n",
      "[ 0.77794296 -0.6569155 ] 1\n",
      "[1.93703593 0.00637656] 1\n",
      "[1.68102605 0.05928866] 1\n",
      "[ 0.97975781 -0.4878307 ] 1\n",
      "[ 1.3612154  -0.40184742] 1\n",
      "[0.4077124  0.55428844] 0\n",
      "[-0.77586432  0.31424212] 0\n",
      "[ 1.62482696 -0.46157648] 1\n",
      "[0.85515204 0.60051796] 0\n",
      "[-0.89879126  0.34577398] 0\n",
      "[-0.41201813  0.79015051] 0\n",
      "[0.06120277 1.10957802] 0\n",
      "[-0.75078621  0.60496757] 0\n",
      "[ 0.81018234 -0.75984919] 1\n",
      "[0.10612906 0.11252792] 1\n",
      "[ 0.63816763 -0.48387763] 1\n",
      "[ 1.42910403 -0.11519178] 1\n",
      "[0.58360795 0.73878271] 0\n",
      "[0.4996589  0.68574501] 0\n",
      "[ 1.15705903 -0.46717375] 1\n",
      "[ 0.01506469 -0.0636001 ] 1\n",
      "[-0.80104889  0.72518126] 0\n",
      "[ 0.92689267 -0.42208605] 1\n",
      "[-0.56621905  0.53797779] 0\n",
      "[-0.28722299  0.21548694] 1\n",
      "[ 0.82621581 -0.54643609] 1\n",
      "[0.60164498 0.98266649] 0\n",
      "[0.68284542 0.70266714] 0\n",
      "[1.84538498 0.0249988 ] 1\n",
      "[1.75222978 0.03556439] 1\n",
      "[ 1.07650655 -0.11795233] 0\n",
      "[1.8518576  0.30267734] 1\n",
      "[ 1.00003422 -0.65185052] 1\n",
      "[0.27512775 0.83959699] 0\n",
      "[1.51920708 0.10849215] 1\n",
      "[0.97621523 0.50078379] 0\n",
      "[0.4418464 0.7568234] 0\n",
      "[-0.74041625  0.46967574] 0\n",
      "[-0.31153252  1.33368258] 0\n",
      "[-0.00212322  0.94003204] 0\n",
      "[1.81971629 0.28836487] 1\n",
      "[ 0.88103261 -0.14496871] 1\n",
      "[0.05968097 1.27037972] 0\n",
      "[ 0.84087967 -0.18182257] 0\n",
      "[-0.83927489  0.23297248] 0\n",
      "[1.0402795  1.00172047] 0\n",
      "[1.74904552 0.05346788] 1\n",
      "[-0.40589106  1.022389  ] 0\n",
      "[ 1.51422577 -0.25937108] 1\n",
      "[-0.11872732  0.87673287] 0\n",
      "[0.34148113 0.28065984] 1\n",
      "[ 0.44738987 -0.52301772] 1\n",
      "[-0.89488778  0.3656674 ] 0\n",
      "[0.89027585 0.9010266 ] 0\n",
      "[-1.24820216  0.24053513] 0\n",
      "[-0.34138427  0.91993731] 0\n",
      "[-1.24204503 -0.31217533] 0\n",
      "[ 0.78807126 -0.42433313] 1\n",
      "[ 1.05930966 -0.91723284] 1\n",
      "[ 1.4712536 -0.6061935] 1\n",
      "[0.581011  1.0143185] 0\n",
      "[2.06376055 0.29688683] 1\n",
      "[ 0.85479896 -0.2651885 ] 1\n",
      "[ 1.24875159 -0.43012529] 0\n",
      "[ 1.42747616 -0.34429635] 1\n",
      "[ 1.93381771 -0.18145371] 1\n",
      "[ 0.32479472 -0.07371328] 1\n",
      "[0.18533318 1.04301368] 0\n",
      "[1.82263032 0.04417412] 1\n",
      "[1.84126437 0.37481721] 1\n",
      "[0.64723796 1.06913556] 0\n",
      "[0.24329863 0.31121472] 1\n",
      "[-1.00189241  0.19076642] 0\n",
      "[-0.46360287  0.81014727] 0\n",
      "[0.50489847 0.07343007] 1\n",
      "[-0.16914705  1.23805511] 0\n",
      "[0.85409176 0.10302146] 0\n",
      "[ 1.36747522 -0.60416914] 1\n",
      "[ 0.72025728 -0.44668772] 1\n",
      "[0.0210309  0.44885741] 1\n",
      "[2.01695053 0.52988681] 1\n",
      "[ 1.05408653 -0.23597393] 1\n",
      "[-0.30955833  0.76888819] 0\n",
      "[ 1.5520625  -0.04260239] 1\n",
      "[0.42826607 0.74838135] 0\n",
      "[0.8007621  0.44224938] 0\n",
      "[-0.63607889  0.83735108] 0\n",
      "[-1.26025872  0.33714688] 0\n",
      "[0.81978497 0.47821312] 0\n",
      "[ 1.19431921 -0.50051398] 1\n",
      "[ 0.71202691 -0.69290291] 1\n",
      "[0.35307519 0.08043878] 1\n",
      "[0.30081141 0.94903473] 0\n",
      "[ 1.98116206 -0.09155386] 1\n",
      "[0.52511281 0.84755776] 0\n",
      "[0.16216882 1.17167431] 0\n",
      "[ 1.8893449 -0.4038114] 1\n",
      "[2.12922584 0.27172577] 1\n",
      "[ 0.81012248 -0.47183124] 1\n",
      "[0.98968139 0.44849833] 0\n",
      "[0.65672132 0.51219752] 0\n",
      "[0.89459903 0.32620774] 0\n",
      "[-0.37191868  0.99358505] 0\n",
      "[ 1.92387451 -0.10488459] 1\n",
      "[-0.75675598  0.79891073] 0\n",
      "[ 1.2317117  -1.03139366] 1\n",
      "[-0.1829159  -0.13512572] 1\n",
      "[-0.99202816  0.13818166] 0\n",
      "[-0.09596932  0.94918697] 1\n",
      "[0.32468592 1.12053418] 0\n",
      "[-0.02340158 -0.23895947] 1\n",
      "[-0.50446269  0.84103539] 0\n",
      "[0.88710062 0.34961984] 0\n",
      "[-0.49380273  0.66798007] 0\n",
      "[0.5284416 0.8828594] 0\n",
      "[-0.36201885  1.02387247] 0\n",
      "[ 1.28924207 -0.45635409] 1\n",
      "[0.60806558 0.9342653 ] 0\n",
      "[ 0.3819977  -0.06149683] 1\n",
      "[0.01841434 1.09988184] 0\n",
      "[ 1.33624915 -0.64084203] 1\n",
      "[-0.02427266  0.06382021] 1\n",
      "[1.10388446 0.01953757] 0\n",
      "[0.93790788 0.45087641] 0\n",
      "[ 0.77938161 -0.0222592 ] 0\n",
      "[0.83508997 0.58432141] 0\n",
      "[0.12886449 1.20130289] 0\n",
      "[0.25510426 0.28784852] 1\n",
      "[-0.92738353  1.05717623] 0\n",
      "[-0.58516891  0.94740686] 0\n",
      "[0.33474261 1.42285782] 0\n",
      "[1.11753122 0.50046466] 0\n",
      "[2.16147579 0.08259556] 1\n",
      "[-0.70414922 -0.01955611] 0\n",
      "[0.17632976 0.04694077] 1\n",
      "[0.2043957 0.5898531] 0\n",
      "[0.8867218  0.25185228] 0\n",
      "[-0.9859334   0.19289109] 0\n",
      "[ 0.78816266 -0.77449036] 1\n",
      "[-0.01983169 -0.31307305] 1\n",
      "[ 0.98051245 -0.38414918] 1\n",
      "[-0.81507781  0.49127931] 0\n",
      "[2.23900764 0.57148476] 1\n",
      "[ 1.49223594 -0.68245593] 1\n",
      "[0.24975856 0.30876198] 0\n",
      "[-1.25200519  0.55069785] 0\n",
      "[ 0.69679642 -0.26283836] 1\n",
      "[-0.83028996  0.78200234] 0\n",
      "[ 1.58193942 -0.22692219] 1\n",
      "[ 1.14865454 -0.53450838] 1\n",
      "[0.73513801 0.4065181 ] 0\n",
      "[-0.15166809  0.19873998] 1\n",
      "[-0.2830165  0.1735996] 1\n",
      "[0.42698266 0.05540425] 1\n",
      "[1.80495911 0.17563308] 1\n",
      "[2.05481867 0.40309354] 1\n",
      "[0.25098672 0.03860826] 1\n",
      "[0.98818088 0.41280544] 0\n",
      "[ 0.08523218 -0.12258878] 1\n",
      "[-0.28386557  1.23276274] 0\n",
      "[0.34083599 0.74511322] 1\n",
      "[0.09505557 0.02409739] 1\n",
      "[0.29745156 0.95890758] 0\n",
      "[-0.99655565  0.78340231] 0\n",
      "[ 0.51490915 -0.38894017] 1\n",
      "[ 0.32388873 -0.09858667] 1\n",
      "[ 1.39066005 -0.46018412] 1\n",
      "[-1.36507063  0.40640562] 0\n",
      "[2.2171412  0.30878371] 1\n",
      "[-0.8633229   0.39160841] 0\n",
      "[-0.08284156  0.87774694] 0\n",
      "[0.78696078 0.36742947] 0\n",
      "[ 0.72126524 -0.40342536] 1\n",
      "[1.8622193  0.42738214] 1\n",
      "[-0.34429596  0.6478919 ] 0\n",
      "[0.61835781 0.95238808] 0\n",
      "[0.33624632 0.56843598] 1\n",
      "[ 1.01163682 -0.80541964] 1\n",
      "[0.23652305 0.7981672 ] 0\n",
      "[ 1.73780118 -0.30799544] 1\n",
      "[0.02247114 0.25420449] 1\n",
      "[ 0.54948691 -0.12316341] 0\n",
      "[ 0.8538637  -0.43163782] 1\n",
      "[ 0.10793426 -0.05785921] 1\n",
      "[1.09508482 0.37276725] 0\n",
      "[1.17350271 0.59742861] 0\n",
      "[ 0.23089964 -0.15517088] 1\n",
      "[0.06214799 1.0076965 ] 0\n",
      "[0.70155616 0.52783718] 0\n",
      "[0.18667772 0.20274971] 1\n",
      "[2.16371731 0.53834805] 1\n",
      "[0.01500486 1.21290543] 0\n",
      "[ 1.17989458 -0.65235115] 1\n",
      "[ 0.69610797 -0.43704294] 1\n",
      "[ 0.28410248 -0.06576568] 1\n",
      "[0.85674743 0.7519873 ] 0\n",
      "[-0.11344853  1.09546984] 0\n",
      "[0.1427436  0.74213138] 0\n",
      "[-0.2396428 -0.142272 ] 1\n",
      "[-0.74423406  0.29485603] 0\n",
      "[ 0.22344381 -0.39133588] 1\n",
      "[0.05246221 1.07050733] 0\n",
      "[1.69920866 0.19488764] 1\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.permutation(X_train.shape[0])\n",
    "for i in idx:\n",
    "    print(X_train[i], y_train[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (1,) and (5,) not aligned: 1 (dim 0) != 5 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m myNN \u001b[39m=\u001b[39m myNeuralNetwork(n_in\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, n_layer1\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_layer2\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_out\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Train the model and collect the cost values for each epoch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m training_loss, validation_loss \u001b[39m=\u001b[39m myNN\u001b[39m.\u001b[39;49mfit(X_train, y_train, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, get_validation_loss\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[23], line 176\u001b[0m, in \u001b[0;36mmyNeuralNetwork.fit\u001b[0;34m(self, X, y, max_epochs, learning_rate, get_validation_loss)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[39m# iterate through mini-batches\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m# for epoch in range(max_epochs):\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m#     for i in range(0, X_train.shape[0], batch_size):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39m#         for x, y in zip(x_batch, y_batch):\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[39m#             self.backpropagate(x, y)\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[0;32m--> 176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstochastic_gradient_descent_step()\n\u001b[1;32m    177\u001b[0m     training_loss\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(X_train, y_train))\n\u001b[1;32m    179\u001b[0m     \u001b[39mif\u001b[39;00m get_validation_loss:\n",
      "Cell \u001b[0;32mIn[23], line 141\u001b[0m, in \u001b[0;36mmyNeuralNetwork.stochastic_gradient_descent_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m idx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mpermutation(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mX_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    140\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx:\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackpropagate(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX_train[i], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_train[i])\n",
      "Cell \u001b[0;32mIn[23], line 112\u001b[0m, in \u001b[0;36mmyNeuralNetwork.backpropagate\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    110\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_propagation(x)\n\u001b[1;32m    111\u001b[0m delta3 \u001b[39m=\u001b[39m (y \u001b[39m-\u001b[39m y_hat) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid_derivative(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma3)\n\u001b[0;32m--> 112\u001b[0m NW3 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(delta3,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mz2\u001b[39m.\u001b[39;49mT)\n\u001b[1;32m    113\u001b[0m \u001b[39m# hidden layer 2 -> hidden layer 1\u001b[39;00m\n\u001b[1;32m    114\u001b[0m delta2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW3\u001b[39m.\u001b[39mT,delta3)\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid_derivative(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma2)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (1,) and (5,) not aligned: 1 (dim 0) != 5 (dim 0)"
     ]
    }
   ],
   "source": [
    "myNN = myNeuralNetwork(n_in=2, n_layer1=5, n_layer2=5,\n",
    "                       n_out=1, learning_rate=0.01)\n",
    "\n",
    "# Train the model and collect the cost values for each epoch\n",
    "training_loss, validation_loss = myNN.fit(\n",
    "    X_train, y_train, max_epochs=1000, learning_rate=0.01, get_validation_loss=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(training_loss, label=\"Training loss\")\n",
    "plt.plot(validation_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = X_train\n",
    "X.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_loss, label=\"Training loss\")\n",
    "plt.plot(validation_loss, label=\"Validation loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the combined training and validation sets with the best hyperparameters\n",
    "clf = myNeuralNetwork(n_in=2, n_layer1=5, n_layer2=5,\n",
    "                      n_out=1, learning_rate=0.1)\n",
    "clf.fit(np.concatenate([X_train, X_val]),\n",
    "        np.concatenate([y_train, y_val]), max_epochs=2000)\n",
    "\n",
    "# Create a 2x1 subplot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "\n",
    "# Plot the decision boundary on the training data\n",
    "scatter0 = axs[0].scatter(X_train[:, 0], X_train[:, 1],\n",
    "                          c=y_train, cmap=plt.cm.Spectral)\n",
    "axs[0].set_title('Decision boundary for training data')\n",
    "xx, yy = np.meshgrid(np.linspace(-2, 3, 100), np.linspace(-2, 2, 100))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "contour0 = axs[0].contourf(\n",
    "    xx, yy, Z, levels=[0, 0.5, 1], alpha=0.5, cmap=plt.cm.Spectral)\n",
    "axs[0].contour(xx, yy, Z, levels=[0.5], colors='k')\n",
    "axs[0].set_xticks([])\n",
    "axs[0].set_yticks([])\n",
    "\n",
    "# Plot the decision boundary on the validation data\n",
    "scatter1 = axs[1].scatter(X_val[:, 0], X_val[:, 1],\n",
    "                          c=y_val, cmap=plt.cm.Spectral)\n",
    "axs[1].set_title('Decision boundary for validation data')\n",
    "contour1 = axs[1].contourf(\n",
    "    xx, yy, Z, levels=[0, 0.5, 1], alpha=0.5, cmap=plt.cm.Spectral)\n",
    "axs[1].contour(xx, yy, Z, levels=[0.5], colors='k')\n",
    "axs[1].set_xticks([])\n",
    "axs[1].set_yticks([])\n",
    "\n",
    "# Create legend\n",
    "handles = [scatter0.legend_elements()[0][0], scatter0.legend_elements()[0][1]]\n",
    "labels = ['Class 1', 'Class 0']\n",
    "fig.legend(handles, labels, loc='center')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (4,2) and (400,2) not aligned: 2 (dim 1) != 400 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 153\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39mreturn\u001b[39;00m params_values, cost_history, accuracy_history\n\u001b[1;32m    152\u001b[0m \u001b[39m# test the model\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m params_values, cost_history, accuracy_history \u001b[39m=\u001b[39m train(X_train, y_train, nn_architecture, \u001b[39m1000\u001b[39;49m, \u001b[39m0.01\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[92], line 140\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X, Y, nn_architecture, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    137\u001b[0m accuracy_history \u001b[39m=\u001b[39m []\n\u001b[1;32m    139\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m--> 140\u001b[0m     Y_hat, cashe \u001b[39m=\u001b[39m full_forward_propagation(X, params_values, nn_architecture)\n\u001b[1;32m    141\u001b[0m     cost \u001b[39m=\u001b[39m get_cost_value(Y_hat, Y)\n\u001b[1;32m    142\u001b[0m     cost_history\u001b[39m.\u001b[39mappend(cost)\n",
      "Cell \u001b[0;32mIn[92], line 66\u001b[0m, in \u001b[0;36mfull_forward_propagation\u001b[0;34m(X, params_values, nn_architecture)\u001b[0m\n\u001b[1;32m     64\u001b[0m W_curr \u001b[39m=\u001b[39m params_values[\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer_idx)]\n\u001b[1;32m     65\u001b[0m b_curr \u001b[39m=\u001b[39m params_values[\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer_idx)]\n\u001b[0;32m---> 66\u001b[0m A_curr, Z_curr \u001b[39m=\u001b[39m single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n\u001b[1;32m     68\u001b[0m memory[\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(idx)] \u001b[39m=\u001b[39m A_prev\n\u001b[1;32m     69\u001b[0m memory[\u001b[39m\"\u001b[39m\u001b[39mZ\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer_idx)] \u001b[39m=\u001b[39m Z_curr\n",
      "Cell \u001b[0;32mIn[92], line 43\u001b[0m, in \u001b[0;36msingle_layer_forward_propagation\u001b[0;34m(A_prev, W_curr, b_curr, activation)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingle_layer_forward_propagation\u001b[39m(A_prev, W_curr, b_curr, activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 43\u001b[0m     Z_curr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(W_curr, A_prev) \u001b[39m+\u001b[39m b_curr\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m activation \u001b[39mis\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m         activation_func \u001b[39m=\u001b[39m relu\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,2) and (400,2) not aligned: 2 (dim 1) != 400 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "\n",
    "def init_layers(nn_architecture, seed=99):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "\n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "\n",
    "    return params_values\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy=True)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ\n",
    "\n",
    "\n",
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "\n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "\n",
    "    return activation_func(Z_curr), Z_curr\n",
    "\n",
    "\n",
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(\n",
    "            A_prev, W_curr, b_curr, activ_function_curr)\n",
    "\n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "\n",
    "    return A_curr, memory\n",
    "\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    m = Y_hat.shape[1]\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) +\n",
    "                     np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()\n",
    "\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "\n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr\n",
    "\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "\n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n",
    "\n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "\n",
    "        dA_curr = dA_prev\n",
    "\n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "\n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "\n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "\n",
    "    return grads_values\n",
    "\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    for layer_idx, layer in enumerate(nn_architecture):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * \\\n",
    "            grads_values[\"dW\" + str(layer_idx)]\n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * \\\n",
    "            grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values\n",
    "\n",
    "\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        Y_hat, cashe = full_forward_propagation(\n",
    "            X, params_values, nn_architecture)\n",
    "        cost = get_cost_value(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        accuracy = get_accuracy_value(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "\n",
    "        grads_values = full_backward_propagation(\n",
    "            Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        params_values = update(params_values, grads_values,\n",
    "                               nn_architecture, learning_rate)\n",
    "\n",
    "    return params_values, cost_history, accuracy_history\n",
    "\n",
    "\n",
    "# test the model\n",
    "params_values, cost_history, accuracy_history = train(\n",
    "    X_train, y_train, nn_architecture, 1000, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
