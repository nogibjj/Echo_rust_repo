{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create training, validation, and test datasets\n",
    "N_train = 500\n",
    "N_test = 100\n",
    "X, y = make_moons(N_train + N_test, noise=0.20, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=N_test, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (5,2) and (400,2) not aligned: 2 (dim 1) != 400 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m myNN \u001b[39m=\u001b[39m myNeuralNetwork(n_in\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, n_layer1\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_layer2\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_out\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[39m# Train the model and collect the cost values for each epoch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m training_loss, validation_loss \u001b[39m=\u001b[39m myNN\u001b[39m.\u001b[39;49mfit(X_train, y_train, max_epochs\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m, learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, get_validation_loss\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[66], line 179\u001b[0m, in \u001b[0;36mmyNeuralNetwork.fit\u001b[0;34m(self, X, y, max_epochs, learning_rate, get_validation_loss)\u001b[0m\n\u001b[1;32m    176\u001b[0m X_train, X_val, y_train, y_val \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[1;32m    178\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(max_epochs):\n\u001b[0;32m--> 179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstochastic_gradient_descent_step()\n\u001b[1;32m    180\u001b[0m     training_loss\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(X_train, y_train))\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m get_validation_loss:\n",
      "Cell \u001b[0;32mIn[66], line 159\u001b[0m, in \u001b[0;36mmyNeuralNetwork.stochastic_gradient_descent_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m y_train \u001b[39m=\u001b[39m df[:,[\u001b[39m2\u001b[39m]]\n\u001b[1;32m    158\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_train)):\n\u001b[0;32m--> 159\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackpropagate(X_train[i,:]\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m), y_train[i,:])\n",
      "Cell \u001b[0;32mIn[66], line 135\u001b[0m, in \u001b[0;36mmyNeuralNetwork.backpropagate\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW1 \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate \u001b[39m*\u001b[39m dW1\n\u001b[1;32m    134\u001b[0m \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mX_train,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49my_train)\n\u001b[1;32m    136\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[66], line 93\u001b[0m, in \u001b[0;36mmyNeuralNetwork.compute_loss\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''compute_loss\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39mComputes the current loss/cost function of the neural network\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39mbased on the weights and the data input into this function.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m        loss: a scalar measure of loss/cost\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m# another way to compute the loss\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# mean squared error loss function\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m# y_hat = self.forward_propagation(X)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m# y_hat = np.array(y_hat).reshape(-1,1)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39m# y = y.reshape(-1,1)\u001b[39;00m\n\u001b[0;32m---> 93\u001b[0m y_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_propagation(X)\n\u001b[1;32m     94\u001b[0m loss \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(\u001b[39m-\u001b[39my \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog(y_hat) \u001b[39m-\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m y) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m y_hat))\n\u001b[1;32m     95\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "Cell \u001b[0;32mIn[66], line 45\u001b[0m, in \u001b[0;36mmyNeuralNetwork.forward_propagation\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m \u001b[39m\u001b[39m'''forward_propagation\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m Takes a vector of your input data (one sample) and feeds\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m it forward through the neural network, calculating activations and\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39m         (typically n_out will be 1 for binary classification)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m '''\u001b[39;00m\n\u001b[1;32m     44\u001b[0m  \u001b[39m# input layer -> hidden layer 1\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma1 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mW1,x)\n\u001b[1;32m     46\u001b[0m  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msigmoid(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma1)\n\u001b[1;32m     48\u001b[0m \u001b[39m# hidden layer 1 -> hidden layer 2\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (5,2) and (400,2) not aligned: 2 (dim 1) != 400 (dim 0)"
     ]
    }
   ],
   "source": [
    "myNN = myNeuralNetwork(n_in=2, n_layer1=5, n_layer2=5, n_out=1, learning_rate=0.01)\n",
    "\n",
    "# Train the model and collect the cost values for each epoch\n",
    "training_loss, validation_loss = myNN.fit(X_train, y_train, max_epochs=1000, learning_rate=0.01, get_validation_loss=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model on the combined training and validation sets with the best hyperparameters\n",
    "clf = myNeuralNetwork(n_in=2, n_layer1=5, n_layer2=5, n_out=1, learning_rate=0.1)\n",
    "clf.fit(np.concatenate([X_train, X_val]), np.concatenate([y_train, y_val]),max_epochs=2000)\n",
    "\n",
    "# Create a 2x1 subplot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "\n",
    "# Plot the decision boundary on the training data\n",
    "scatter0 = axs[0].scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.Spectral)\n",
    "axs[0].set_title('Decision boundary for training data')\n",
    "xx, yy = np.meshgrid(np.linspace(-2, 3, 100), np.linspace(-2, 2, 100))\n",
    "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "contour0 = axs[0].contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.5, cmap=plt.cm.Spectral)\n",
    "axs[0].contour(xx, yy, Z, levels=[0.5], colors='k')\n",
    "axs[0].set_xticks([])\n",
    "axs[0].set_yticks([])\n",
    "\n",
    "# Plot the decision boundary on the validation data\n",
    "scatter1 = axs[1].scatter(X_val[:, 0], X_val[:, 1], c=y_val, cmap=plt.cm.Spectral)\n",
    "axs[1].set_title('Decision boundary for validation data')\n",
    "contour1 = axs[1].contourf(xx, yy, Z, levels=[0, 0.5, 1], alpha=0.5, cmap=plt.cm.Spectral)\n",
    "axs[1].contour(xx, yy, Z, levels=[0.5], colors='k')\n",
    "axs[1].set_xticks([])\n",
    "axs[1].set_yticks([])\n",
    "\n",
    "# Create legend\n",
    "handles = [scatter0.legend_elements()[0][0], scatter0.legend_elements()[0][1]]\n",
    "labels = ['Class 1', 'Class 0']\n",
    "fig.legend(handles, labels, loc='center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:45: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:47: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n",
      "/tmp/ipykernel_2184/279329428.py:87: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if activation is \"relu\":\n",
      "/tmp/ipykernel_2184/279329428.py:89: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  elif activation is \"sigmoid\":\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (4,2) and (400,2) not aligned: 2 (dim 1) != 400 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 153\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39mreturn\u001b[39;00m params_values, cost_history, accuracy_history\n\u001b[1;32m    152\u001b[0m \u001b[39m# test the model\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m params_values, cost_history, accuracy_history \u001b[39m=\u001b[39m train(X_train, y_train, nn_architecture, \u001b[39m1000\u001b[39;49m, \u001b[39m0.01\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[92], line 140\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X, Y, nn_architecture, epochs, learning_rate)\u001b[0m\n\u001b[1;32m    137\u001b[0m accuracy_history \u001b[39m=\u001b[39m []\n\u001b[1;32m    139\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m--> 140\u001b[0m     Y_hat, cashe \u001b[39m=\u001b[39m full_forward_propagation(X, params_values, nn_architecture)\n\u001b[1;32m    141\u001b[0m     cost \u001b[39m=\u001b[39m get_cost_value(Y_hat, Y)\n\u001b[1;32m    142\u001b[0m     cost_history\u001b[39m.\u001b[39mappend(cost)\n",
      "Cell \u001b[0;32mIn[92], line 66\u001b[0m, in \u001b[0;36mfull_forward_propagation\u001b[0;34m(X, params_values, nn_architecture)\u001b[0m\n\u001b[1;32m     64\u001b[0m W_curr \u001b[39m=\u001b[39m params_values[\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer_idx)]\n\u001b[1;32m     65\u001b[0m b_curr \u001b[39m=\u001b[39m params_values[\u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer_idx)]\n\u001b[0;32m---> 66\u001b[0m A_curr, Z_curr \u001b[39m=\u001b[39m single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n\u001b[1;32m     68\u001b[0m memory[\u001b[39m\"\u001b[39m\u001b[39mA\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(idx)] \u001b[39m=\u001b[39m A_prev\n\u001b[1;32m     69\u001b[0m memory[\u001b[39m\"\u001b[39m\u001b[39mZ\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(layer_idx)] \u001b[39m=\u001b[39m Z_curr\n",
      "Cell \u001b[0;32mIn[92], line 43\u001b[0m, in \u001b[0;36msingle_layer_forward_propagation\u001b[0;34m(A_prev, W_curr, b_curr, activation)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msingle_layer_forward_propagation\u001b[39m(A_prev, W_curr, b_curr, activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> 43\u001b[0m     Z_curr \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(W_curr, A_prev) \u001b[39m+\u001b[39m b_curr\n\u001b[1;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m activation \u001b[39mis\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     46\u001b[0m         activation_func \u001b[39m=\u001b[39m relu\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (4,2) and (400,2) not aligned: 2 (dim 1) != 400 (dim 0)"
     ]
    }
   ],
   "source": [
    "nn_architecture = [\n",
    "    {\"input_dim\": 2, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 6, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 6, \"output_dim\": 4, \"activation\": \"relu\"},\n",
    "    {\"input_dim\": 4, \"output_dim\": 1, \"activation\": \"sigmoid\"},\n",
    "]\n",
    "\n",
    "def init_layers(nn_architecture, seed = 99):\n",
    "    np.random.seed(seed)\n",
    "    number_of_layers = len(nn_architecture)\n",
    "    params_values = {}\n",
    "\n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        layer_input_size = layer[\"input_dim\"]\n",
    "        layer_output_size = layer[\"output_dim\"]\n",
    "        \n",
    "        params_values['W' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, layer_input_size) * 0.1\n",
    "        params_values['b' + str(layer_idx)] = np.random.randn(\n",
    "            layer_output_size, 1) * 0.1\n",
    "        \n",
    "    return params_values\n",
    "\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0,Z)\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    sig = sigmoid(Z)\n",
    "    return dA * sig * (1 - sig)\n",
    "\n",
    "def relu_backward(dA, Z):\n",
    "    dZ = np.array(dA, copy = True)\n",
    "    dZ[Z <= 0] = 0;\n",
    "    return dZ;\n",
    "\n",
    "def single_layer_forward_propagation(A_prev, W_curr, b_curr, activation=\"relu\"):\n",
    "    Z_curr = np.dot(W_curr, A_prev) + b_curr\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        activation_func = relu\n",
    "    elif activation is \"sigmoid\":\n",
    "        activation_func = sigmoid\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "        \n",
    "    return activation_func(Z_curr), Z_curr\n",
    "\n",
    "\n",
    "def full_forward_propagation(X, params_values, nn_architecture):\n",
    "    memory = {}\n",
    "    A_curr = X\n",
    "    \n",
    "    for idx, layer in enumerate(nn_architecture):\n",
    "        layer_idx = idx + 1\n",
    "        A_prev = A_curr\n",
    "        \n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx)]\n",
    "        A_curr, Z_curr = single_layer_forward_propagation(A_prev, W_curr, b_curr, activ_function_curr)\n",
    "        \n",
    "        memory[\"A\" + str(idx)] = A_prev\n",
    "        memory[\"Z\" + str(layer_idx)] = Z_curr\n",
    "       \n",
    "    return A_curr, memory\n",
    "\n",
    "\n",
    "def get_cost_value(Y_hat, Y):\n",
    "    m = Y_hat.shape[1]\n",
    "    cost = -1 / m * (np.dot(Y, np.log(Y_hat).T) + np.dot(1 - Y, np.log(1 - Y_hat).T))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "def get_accuracy_value(Y_hat, Y):\n",
    "    Y_hat_ = convert_prob_into_class(Y_hat)\n",
    "    return (Y_hat_ == Y).all(axis=0).mean()\n",
    "\n",
    "\n",
    "def single_layer_backward_propagation(dA_curr, W_curr, b_curr, Z_curr, A_prev, activation=\"relu\"):\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    if activation is \"relu\":\n",
    "        backward_activation_func = relu_backward\n",
    "    elif activation is \"sigmoid\":\n",
    "        backward_activation_func = sigmoid_backward\n",
    "    else:\n",
    "        raise Exception('Non-supported activation function')\n",
    "    \n",
    "    dZ_curr = backward_activation_func(dA_curr, Z_curr)\n",
    "    dW_curr = np.dot(dZ_curr, A_prev.T) / m\n",
    "    db_curr = np.sum(dZ_curr, axis=1, keepdims=True) / m\n",
    "    dA_prev = np.dot(W_curr.T, dZ_curr)\n",
    "\n",
    "    return dA_prev, dW_curr, db_curr\n",
    "\n",
    "def full_backward_propagation(Y_hat, Y, memory, params_values, nn_architecture):\n",
    "    grads_values = {}\n",
    "    m = Y.shape[1]\n",
    "    Y = Y.reshape(Y_hat.shape)\n",
    "   \n",
    "    dA_prev = - (np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat));\n",
    "    \n",
    "    for layer_idx_prev, layer in reversed(list(enumerate(nn_architecture))):\n",
    "        layer_idx_curr = layer_idx_prev + 1\n",
    "        activ_function_curr = layer[\"activation\"]\n",
    "        \n",
    "        dA_curr = dA_prev\n",
    "        \n",
    "        A_prev = memory[\"A\" + str(layer_idx_prev)]\n",
    "        Z_curr = memory[\"Z\" + str(layer_idx_curr)]\n",
    "        W_curr = params_values[\"W\" + str(layer_idx_curr)]\n",
    "        b_curr = params_values[\"b\" + str(layer_idx_curr)]\n",
    "        \n",
    "        dA_prev, dW_curr, db_curr = single_layer_backward_propagation(\n",
    "            dA_curr, W_curr, b_curr, Z_curr, A_prev, activ_function_curr)\n",
    "        \n",
    "        grads_values[\"dW\" + str(layer_idx_curr)] = dW_curr\n",
    "        grads_values[\"db\" + str(layer_idx_curr)] = db_curr\n",
    "    \n",
    "    return grads_values\n",
    "\n",
    "def update(params_values, grads_values, nn_architecture, learning_rate):\n",
    "    for layer_idx, layer in enumerate(nn_architecture):\n",
    "        params_values[\"W\" + str(layer_idx)] -= learning_rate * grads_values[\"dW\" + str(layer_idx)]        \n",
    "        params_values[\"b\" + str(layer_idx)] -= learning_rate * grads_values[\"db\" + str(layer_idx)]\n",
    "\n",
    "    return params_values;\n",
    "\n",
    "def train(X, Y, nn_architecture, epochs, learning_rate):\n",
    "    params_values = init_layers(nn_architecture, 2)\n",
    "    cost_history = []\n",
    "    accuracy_history = []\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        Y_hat, cashe = full_forward_propagation(X, params_values, nn_architecture)\n",
    "        cost = get_cost_value(Y_hat, Y)\n",
    "        cost_history.append(cost)\n",
    "        accuracy = get_accuracy_value(Y_hat, Y)\n",
    "        accuracy_history.append(accuracy)\n",
    "        \n",
    "        grads_values = full_backward_propagation(Y_hat, Y, cashe, params_values, nn_architecture)\n",
    "        params_values = update(params_values, grads_values, nn_architecture, learning_rate)\n",
    "        \n",
    "    return params_values, cost_history, accuracy_history\n",
    "\n",
    "\n",
    "# test the model\n",
    "params_values, cost_history, accuracy_history = train(X_train, y_train, nn_architecture, 1000, 0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
